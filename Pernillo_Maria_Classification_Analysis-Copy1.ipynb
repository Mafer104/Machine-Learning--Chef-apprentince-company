{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd                                   # data science essentials\n",
    "import matplotlib.pyplot as plt                       # essential graphical output\n",
    "import seaborn as sns                                 # enhanced graphical output                              \n",
    "import numpy as np                                    # mathematical essentials                    \n",
    "import statsmodels.formula.api as smf                 # regression modeling\n",
    "import sklearn.linear_model                           # linear models\n",
    "import random as rand     \n",
    "\n",
    "# Importing tools from scikitlearn\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "\n",
    "# CART Model Packages\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from six import StringIO                             # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend\n",
    "\n",
    "\n",
    "# New Packages\n",
    "from sklearn.model_selection import RandomizedSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                    # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier        # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier    # gbm\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# loading data\n",
    "chef = pd.read_excel('./datasets/chef_complete.xlsx')\n",
    "\n",
    "# displaying the head of the dataset\n",
    "chef.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# optimal_neighbors\n",
    "########################################\n",
    "def optimal_neighbors(x_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "x_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing x_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(x_data)\n",
    "        x_scaled           = scaler.transform(x_data)\n",
    "        x_scaled_df        = pd.DataFrame(x_scaled)\n",
    "        x_data             = x_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(x_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(x_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800, export = False):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    export     : bool, defalut False\n",
    "        whether or not to export the tree as a .png file\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width,\n",
    "                unconfined = True)\n",
    "\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating initial correlation matrix \n",
    "df_corr = chef.corr().round(2)\n",
    "\n",
    "\n",
    "#Printing correlations with dependent variable\n",
    "\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory variables and the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "col_drop = ['CROSS_SELL_SUCCESS','NAME','FIRST_NAME', 'FAMILY_NAME',\n",
    "            'EMAIL', 'email_domain']\n",
    "\n",
    "chef_data = chef.drop(col_drop, axis = 1)\n",
    "\n",
    "# declaring response variable\n",
    "chef_target = chef.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in chef_data:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory variable sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', \n",
    "                   'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED', \n",
    "                   'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', \n",
    "                   'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON', \n",
    "                   'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'MOBILE_LOGINS', \n",
    "                   'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES', \n",
    "                   'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', \n",
    "                   'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED', \n",
    "                   'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED', \n",
    "                   'number_of_names', 'high_new_customer_engagement', 'meal_rating_1', \n",
    "                   'meal_rating_2', 'meal_rating_3', 'meal_rating_4', 'meal_rating_5', \n",
    "                   'existing_users_final_funnel_lo', 'has_master_classes_attended',\n",
    "                   'has_weekly_plan', 'has_tastes_and_preferences', 'has_cancellations_before_noon', \n",
    "                   'has_cancellations_after_noon', 'has_early_deliveries', 'has_late_deliveries', \n",
    "                   'Company_email', 'Invalid', 'Personal'],\n",
    "\n",
    "            \n",
    "# significant variables only (set 1)\n",
    " 'logit_sig'    : ['number_of_names','CANCELLATIONS_BEFORE_NOON','Company_email',\n",
    "                   'TASTES_AND_PREFERENCES', 'REFRIGERATED_LOCKER', 'Personal'],\n",
    " \n",
    " # significant variables only (set 2)   \n",
    " 'logit_sig_2'    : ['MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON',\n",
    "                    'TASTES_AND_PREFERENCES', 'PC_LOGINS',\n",
    "                    'EARLY_DELIVERIES', 'number_of_names','high_new_customer_engagement']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "                                        REVENUE + \n",
    "                                         TOTAL_MEALS_ORDERED + \n",
    "                                         UNIQUE_MEALS_PURCH + \n",
    "                                         CONTACTS_W_CUSTOMER_SERVICE + \n",
    "                                         PRODUCT_CATEGORIES_VIEWED + \n",
    "                                         AVG_TIME_PER_SITE_VISIT + \n",
    "                                         MOBILE_NUMBER + \n",
    "                                         CANCELLATIONS_BEFORE_NOON + \n",
    "                                         CANCELLATIONS_AFTER_NOON + \n",
    "                                         TASTES_AND_PREFERENCES + \n",
    "                                         PC_LOGINS + \n",
    "                                         MOBILE_LOGINS + \n",
    "                                         WEEKLY_PLAN + \n",
    "                                         EARLY_DELIVERIES + \n",
    "                                         LATE_DELIVERIES + \n",
    "                                         PACKAGE_LOCKER + \n",
    "                                         REFRIGERATED_LOCKER + \n",
    "                                         AVG_PREP_VID_TIME + \n",
    "                                         LARGEST_ORDER_SIZE + \n",
    "                                         MASTER_CLASSES_ATTENDED + \n",
    "                                         MEDIAN_MEAL_RATING + \n",
    "                                         AVG_CLICKS_PER_VISIT + \n",
    "                                         TOTAL_PHOTOS_VIEWED + \n",
    "                                         number_of_names + \n",
    "                                         high_new_customer_engagement +  \n",
    "                                         meal_rating_3 + \n",
    "                                         meal_rating_4 + \n",
    "                                         meal_rating_5 + \n",
    "                                         has_master_classes_attended + \n",
    "                                         has_weekly_plan + \n",
    "                                         has_tastes_and_preferences + \n",
    "                                         has_cancellations_before_noon + \n",
    "                                         has_cancellations_after_noon + \n",
    "                                         has_early_deliveries + \n",
    "                                         has_late_deliveries + \n",
    "                                         Company_email + \n",
    "                                         Invalid + \n",
    "                                         Personal \"\"\",\n",
    "                                         data = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_full = logit_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_sig = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "                                number_of_names +\n",
    "                                CANCELLATIONS_BEFORE_NOON' +\n",
    "                                Company_email +\n",
    "                                TASTES_AND_PREFERENCES +\n",
    "                                REFRIGERATED_LOCKER +\n",
    "                                Personal\"\"\",\n",
    "                                     data    = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_sig = logit_sig.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_sig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_sig_2 = smf.logit(formula =  \"\"\" CROSS_SELL_SUCESS ~\n",
    "                                                            MOBILE_NUMBER + \n",
    "                                                            CANCELLATIONS_BEFORE_NOON +\n",
    "                                                            TASTES_AND_PREFERENCES +\n",
    "                                                            PC_LOGINS +\n",
    "                                                            EARLY_DELIVERIES \"\"\",\n",
    "                                       data    = chef_train)\n",
    " \n",
    "\n",
    "# fitting the model object\n",
    "logit_sig_2 = logit_sig_2.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_sig_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_sig']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tap}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(chef[candidate_dict['logit_sig']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification tree model : Full tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                    y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                    y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification tree model : Pruned_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning on Classification Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 25, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 250, # changed for the sake of time\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(x_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score      TN, FP, FN, TP\n",
    "-----         ---------      --------------\n",
    "Logistic      {logreg_auc_score}          {logreg_tn, logreg_fp, logreg_fn, logreg_tp}\n",
    "Full Tree     {full_tree_auc_score}         {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}\n",
    "Pruned Tree   {pruned_tree_auc_score}         {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic','Full Tree', \n",
    "                       'Pruned Tree', 'Tuned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, \n",
    "                   pruned_tree_auc_score, tree_tuned_auc],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score, tree_tuned_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score, tree_tuned_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp),\n",
    "                          (tuned_tree_tn, tuned_tree_fp, tuned_tree_fn, tuned_tree_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_full variables\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_full'] ]\n",
    "chef_target =  chef.loc [ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = \"gini\",\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 10)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv         = 3,\n",
    "                               n_iter     = 100,\n",
    "                               scoring    = make_scorer(roc_auc_score,\n",
    "                                            needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING with best_estimator\n",
    "forest_tuned = forest_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(x_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(x_test, y_test).round(4))\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
